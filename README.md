<div align="center">

  <img src="Images/SecondMind_PosterPromo.png" width="400" alt="Poster Promo">

  <br><br>
_**"Your system will end up imitating you: clear, orderly, a bit stubborn but reliable."**_

<br>


  ![Python](https://img.shields.io/badge/Python-3.11+-blue?style=for-the-badge&logo=python&logoColor=white)
  ![Hardware](https://img.shields.io/badge/Hardware-RTX%203090-76B900?style=for-the-badge&logo=nvidia&logoColor=white)
  ![Status](https://img.shields.io/badge/Status-Production%20Grade-success?style=for-the-badge)
  ![Focus](https://img.shields.io/badge/Focus-Safety%20by%20Design-red?style=for-the-badge)

  <br>



  <br>

<div align="center">
  <a href="#executive-summary">
    <img src="https://img.shields.io/badge/TL;DR-The_Essentials_in_30_seconds-6f42c1?style=for-the-badge&logo=quicktime&logoColor=white" height="45">
  </a>
</div>



<details>
<summary><b>‚ö° EXECUTIVE SUMMARY (Click to expand)</b></summary>

<br>

### Starting from zero, I designed and developed:
1. **A multi-agent cognitive architecture** Centralized orchestration where each agent has a unique, isolated, and replaceable role.

2. **Strong governance & Traceability**: Implementation of a *Single Source of Truth*, static code audit (AST) and typed interface contracts. Monitoring, logging and statistics.

3. **Hybrid RAG Pipeline**: Combination of Text RAG + Code RAG (dependency graph) with vector and reflexive memory.

4. **Instrumentation via Metaprogramming**: Automatic injection of logs, stats and monitoring into all agents ("Zero-Boilerplate")

5. **Observability**: Creation of a real-time Dashboard (Prompt Viewer) to debug the actual context seen by the LLM.

# **Clear architectural vision**:
- Multi-agent "Hub & Spoke" design: Centralized orchestration (AgentSemi) with loose coupling.
- Modularity: Each agent has a unique role and is an isolated, interchangeable micro-service.
- Strict Contracts: Typed and validated inter-agent communication
- Security by Design: Continuous static audit to guarantee compliance with contracts.
- Fail-Fast & Resilience: Real-time error detection and handling.

# üöÄ **Hardware Optimization**
- Tri-model architecture: SBERT Sentence-Transformers, Phi3Mini and Qwen2.5 Coder
- VRAM optimization: Qwen 2.5 14B inference on consumer GPU (RTX 3090).
- Quantized Cache (Q4/Q8): Management of a 130,000-token window without memory saturation.
- Low Latency: Intention router (SBERT) to avoid unnecessary GPU calls.

<br>

---



<h3 align="center">üìÖ Development Timeline</h3>
This system evolved through rapid iterations, moving from a purely symbolic engine to a neural-governed architecture.

| Phase                                           | Technical Focus                                                                                                                             | Major Achievement                                     |
| :---------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------ | :------------------------------------------------------ |
| **Phase 1: Symbolic Foundations** (Months 1-3) | Development of an NLP engine without LLM and use of FCFG grammars, WordNet and ConceptNet to structure language understanding | Creation of 11 specialized agents in 7 weeks.        |
| **Phase 2: The Neural Pivot** (Month 3)          | Integration of a local LLM (Qwen/Llama).                                                                                                         | Hybridization: Symbolic rigor + Neural flexibility. |
| **Phase 3: Industrialization** (Months 4-6)      | AST analysis, Metaprogramming, VRAM optimization.                                                                                          | "Production-Grade" architecture and strong governance.   |

---

#### üöÄ Current State & Scalability
- [x] **Modular System**: 9 agents, each with a unique role.
- [x] **Governance**: Continuous static audit via `AgentAuditor`.
- [x] **Performance**: 130k token context with Quantized KV Cache.
- [x] **Reliability**: Unitary test strategy with deterministic LLM mocking.
- [ ] **Next Step**: Horizontally scale to image analysis (`agent_Vision`).

</details>

[üîç Project Origin: From Symbolic Brain to LLMs > Discover how SecondMind originated as a deterministic architecture (ConceptNet, Lesk, CFG) before integrating probabilistic models.](./README_symbolic_pipeline.md)

---
‚ö†Ô∏è **IMPORTANT**
>
>
> **Note about source code access:** This repository follows an "Interface-Public / Core-Private" strategy.

> Public: Overall architecture, interface contracts, full technical documentation (READMEs) and unit test suites.

> Private: Agents' business logic and neural implementations.

The full source code is available for an in-depth technical review on request during the recruitment process.


<details>
<summary><b>üì¨ Contact</b></summary>

<div align="center">
  <h3>üì¨ Contact & Collaboration</h3>

  Maxime Gagn√©


  <a href="https://www.linkedin.com/in/maxime-gagn%C3%A9-6b14541b9/">
    <img src="https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white" alt="LinkedIn">
  </a>
  &nbsp;&nbsp;
  <a href="mailto:maximegagne.ai@gmail.com">
    <img src="https://img.shields.io/badge/Email-D14836?style=for-the-badge&logo=gmail&logoColor=white" alt="Email">
  </a>
   <p><i>"Open to opportunities in AI Architecture, Cognitive R&D and Systems Engineering."</i></p>
  <br>
  <blockquote>
    üîí <b>Private repo access:</b> To review the full source code (Core Logic), please send me a request via LinkedIn or email specifying your organization.
  </blockquote>

  </details>


---
---



  # üß† SecondMind | Portfolio of a Cognitive Architect


  ### Local & Self-Governed Cognitive Operating System

</div>



*Six months ago I had no coding or AI experience. Today, SecondMind is my cognitive architecture lab: a complete multi-agent system, governed and reflexive, running locally on an RTX 3090 with a 130k token context.*

---

<div align="center"> <img src="Images/interface_de_chat.png" width="900" alt="Chat Interface"> </div>

---

### üìÇ Project Architecture

```text
SecondMind/
‚îú‚îÄ‚îÄ üìú metabase/              # Affect all agents
‚îÇ   ‚îú‚îÄ‚îÄ META_agent.py         # The Metaclass (Injections for all agents)
‚îÇ   ‚îú‚îÄ‚îÄ contrats_interface.py # Strict typing (Dataclasses)
‚îÇ   ‚îú‚îÄ‚îÄ auditor_base.py       # Guardian of standardization and paths
‚îÇ   ‚îú‚îÄ‚îÄ gardien_projet.py     # Project watchdog manager
‚îÇ   ‚îú‚îÄ‚îÄ cognitive_logger.py   # Logging configuration
‚îú‚îÄ‚îÄ ü§ñ Agentique/             # The 9 Agents
‚îÇ   ‚îú‚îÄ‚îÄ agent_Semi.py         # The Orchestrator
‚îÇ   ‚îú‚îÄ‚îÄ agent_Contexte.py     # Context Manager
‚îÇ   ‚îú‚îÄ‚îÄ agent_Code.py         # Code manager
‚îÇ   ‚îú‚îÄ‚îÄ agent_Recherche.py    # The librarian
‚îÇ   ‚îú‚îÄ‚îÄ agent_Juge.py         # Truth guardian
‚îÇ   ‚îú‚îÄ‚îÄ agent_Parole.py       # Prompt master
‚îÇ   ‚îú‚îÄ‚îÄ agent_Entraineur.py   # Data trainer
‚îÇ   ‚îú‚îÄ‚îÄ agent_Reflexor.py     # Reflexive loop
‚îÇ   ‚îî‚îÄ‚îÄ agent_Auditor.py      # The Police (AST)
‚îú‚îÄ‚îÄ üß† memoire/
‚îÇ   ‚îú‚îÄ‚îÄ brute/                # Interaction JSONL (backup storage)
‚îÇ   ‚îú‚îÄ‚îÄ code/                 # Code extracted from interactions JSONL
‚îÇ   ‚îú‚îÄ‚îÄ connaissances/        # Readmes & Docs
‚îÇ   ‚îú‚îÄ‚îÄ conversations/        # Conversation management
‚îÇ   ‚îú‚îÄ‚îÄ historique/           # Interaction JSONL
‚îÇ   ‚îú‚îÄ‚îÄ persistante/          # Consolidated memories
‚îÇ   ‚îú‚îÄ‚îÄ reflexive/            # Reflexive rules & user feedback
‚îÇ   ‚îî‚îÄ‚îÄ vectorielle/          # Summarized and vectorized interactions
‚îú‚îÄ‚îÄ ‚öôÔ∏è config/                # Single Source of Truth, one YAML per agent
‚îú‚îÄ‚îÄ ‚ú® interfaces/            # Frontend + Backend
‚îÇ   ‚îú‚îÄ‚îÄ interface_de_chat.html
‚îÇ   ‚îú‚îÄ‚îÄ interface_ide.html
‚îÇ   ‚îú‚îÄ‚îÄ interface_benchmark.html
‚îÇ   ‚îú‚îÄ‚îÄ prompt_viewer.html
‚îÇ   ‚îî‚îÄ‚îÄ backend.py
‚îî‚îÄ‚îÄ üöÄ START_SECONDMIND.bat   # Launcher
```

---

### [üîç Focus on the Agent Squad](Docs/Agents/INDEX_AGENTS.md)

The system doesn't rely on a single prompt but on task delegation. It employs a squad of specialized agents, each with a unique responsibility and strict limits (Safeguards).

![Agent table](Images/tableau_des_dependences.drawio.png)

#### ü§ñ [Agent_Semi.py](Agentique/agent_Semi/agent_Semi.py) | [Sp√©cifications](Docs/Agents/README_agent_Semi.md)
**The official SEcondMInd agent, the orchestrator.**
* **Primary responsibility**: It is the "Hub" of decision. It receives the user's intention and coordinates the other agents to build the final response.
* **Philosophy**: Centralization of will, decentralization of execution.

#### üõ°Ô∏è [Agent_Auditor.py](Agentique/agent_Auditor/agent_Auditor.py) | [Sp√©cifications](Docs/Agents/README_agent_Auditor.md)

* **Primary responsibility**: Ensure technical integrity, code security and strict compliance with data contracts across the system.
* **What it never does** :
    * It never modifies business logic or the decision-making behavior of other agents.
    * It never allows destructive actions on sanctified directories without proof of backup rotation.

#### üíª [Agent_Code.py](Agentique/agent_Code/agent_Code.py) | [Sp√©cifications](Docs/Agents/README_agent_Code.md)
* **Primary responsibility**: Orchestrate access to source code intelligence via a hybrid search and indexing system to enable understanding of complex codebases.
* **What it never does** :
    * It never runs unit tests or code in real time.
    * It never modifies source files directly.
    * It must never guess a code structure without querying the index.

#### üß† [Agent_Contexte.py](Agentique/agent_Contexte/agent_Contexte.py) | [Sp√©cifications](Docs/Agents/README_agent_Contexte.md)

* **Primary responsibility**: Orchestrate retrieval, filtering and intelligent formatting of context (memory, rules and history) to prevent conversational amnesia.
* **What it never does** :
    * It never performs raw vector search itself (it delegates that task).
    * It never makes a final decision on the user's response.
    * It must never invent rules not present in the system.

#### ‚öñÔ∏è [Agent_Juge.py](Agentique/agent_Juge/agent_Juge.py) | [Sp√©cifications](Docs/Agents/README_agent_Juge.md)

* **Primary responsibility**: Evaluate quality and factual reliability of system information acting as an unforgiving arbiter of relevance and coherence.
* **What it never does** :
    * It never invents facts to fill gaps in context.
    * It never generates conversational responses intended for the end user.
    * It must never soften a negative verdict for politeness.

#### üîé [Agent_Recherche.py](Agentique/agent_Recherche/agent_Recherche.py) | [Sp√©cifications](Docs/Agents/README_agent_Recherche.md)

* **Primary responsibility**: Locate and extract any relevant information (memory, code, web) with near-instant performance to feed the system's reasoning.
* **What it never does** :
    * It never alters or deletes indexed files.
    * It never generates content without sources (it's an evidence agent).
    * It must never guess a file's content without explicitly reading it.

#### üîÑ [Agent_Reflexor.py](Agentique/agent_Reflexor/agent_Reflexor.py) | [Sp√©cifications](Docs/Agents/README_agent_Reflexor.md)

* **Primary responsibility**: Analyze incidents and user feedback to ensure behavioral self-correction and continuous improvement.
* **What it never does** :
    * It never attempts to justify an error to the user.
    * It never directly modifies executing agents without passing through reflexive logging.
    * It must never erase or hide the system's mistakes.

#### üìù [Agent_Parole.py](Agentique/agent_Parole/agent_Parole.py) | [Sp√©cifications](Docs/Agents/README_agent_Parole.md)

* **Primary responsibility**: Build the final ChatML prompt by dynamically assembling system variables, user profile, and business context to guide the LLM.
* **What it never does** :
    * It never invents content, acting as a pure structure builder.
    * It never communicates directly with the LLM engine.
    * It must never change the tone or identity of Semi.

#### üéì [Agent_Entraineur.py](Agentique/agent_Entraineur/agent_Entraineur.py) | [Sp√©cifications](Docs/Agents/README_agent_Entraineur.md)

* **Primary responsibility**: Oversee system learning by merging datasets and training intention classifiers (SBERT).
* **What it never does** :
    * It never trains the base model (SentenceTransformer), only classifier heads.
    * It never deploys models if validation accuracy is insufficient.
    * It must never be used for real-time intention classification.

#### üíæ [Agent_Memoire.py](Agentique/agent_Memoire/agent_Memoire.py) | [Sp√©cifications](Docs/Agents/README_agent_Memoire.md)

* **Primary responsibility**: Autonomously manage the lifecycle of persistent memory ensuring capture, storage and indexing of all system data.
* **What it never does** :
    * It never deletes data without an explicit governance directive.
    * It never modifies config files or project source code.
    * It must never store temporary or volatile data.

---

## üèóÔ∏è The Metabase: Inheritance & Instrumentation Engine

Rather than coding each agent as an isolated script, I designed a **Metabase** (an internal framework). Each agent is an extension of a base class that automatically injects its capabilities.

### ‚öôÔ∏è [Metaprogramming ‚Äî Zero Boilerplate](Docs/README_metaprogrammation.md)
One early barrier to scaling was technical redundancy. In a ten-agent environment, the conventional approach would require manually initializing fundamental tools in each class: the logger (CognitiveLogger), stats system, auditor and shared RAM access. This repetition, beyond being inelegant, introduced a major inconsistency risk and mental maintenance overhead.

To solve this, I used Python metaprogramming. By designing a metaclass that intercepts the creation of each agent, I automated the injection of dependencies and instrumentation of methods. This engine lets the architect focus only on agent business logic, while the infrastructure (monitoring, traceability and security) is generated invisibly at instantiation.

### üß© [META_agent.py](metabase/META_agent.py) | [Specs](Docs/Systeme/README_META_agent.md) | [Specs](Docs/Systeme/README_stats_manager.md)  ‚Äî The Metaclass
It's the invisible brain of the project. Through Python metaprogramming, it monitors the creation of each agent to:
* **Automatic Injection**: Connect the `Logger`, `Auditor` and `RAM Memory` to the agent without a single extra line of code (Zero-Boilerplate).
* **Contract Surveillance**: Verify the agent respects defined communication standards.
* **Instrumentation**: Measure performance and trace every agent decision in real time.

### üìú [contrats_interface.py](metabase/contrats_interface.py) | [Specs](Docs/Systeme/README_contrats_interface.md) ‚Äî The Common Language
In a multi-agent system, the greatest risk is data inconsistency.
* **Strict Typing**: Use of Python `Dataclasses` to precisely define what an agent can receive and send.
* **Safety**: If an agent tries to send a non-conforming format, the Auditor blocks the exchange immediately (Fail-Fast).

### üõ°Ô∏è [auditor_base.py](metabase/auditor_base.py) | [Specs](Docs/Systeme/README_auditor_base.md) ‚Äî The Source of Truth
The Auditor manages:
* **Paths**: Centralizes locations for all folders (logs, config, memory) to avoid hardcoded paths.
* **Environment Validation**: Ensures hardware (RTX 3090) and dependencies are ready before launching the engine.

### üïµÔ∏è [cognitive_logger.py](metabase/cognitive_logger.py) | [Specs](Docs/Systeme/README_cognitive_logger.md) ‚Äî Cognitive Trace
This logging records the system's "thought":
* **Custom Log Levels**: Distinguish technical logs, orchestration decisions and internal reflections.
* **Traceability**: Each log line is associated with a "session ID", enabling full reconstruction of the system's thought process after the fact.

### üëÅÔ∏è‚Äçüó®Ô∏è [gardien_projet.py](metabase/gardien_projet.py) | [Specs](Docs/Systeme/README_gardien_projet.md) ‚Äî Project Watchdog
A watchdog service based on `watchdog` that ensures sync between source code and system intelligence in real time:
* **Dynamic Re-indexing**: Detects file changes and orders AgentCode to refresh its project view, keeping the technical RAG fresh.
* **Live Security Audit**: Delegates automatic audits to AgentAuditor on each save.
* **Backend Observability**: Periodic synchronization of stats to control interface for constant health monitoring.

### üïµÔ∏è [cognitive_logger.py](metabase/cognitive_logger.py) | [Specs](Docs/Systeme/README_cognitive_logger.md) ‚Äî The Trace Cognitive
The logging here doesn't just show errors. It records the system's thought:
* **Custom log levels**: Distinguish between technical logs, orchestration decisions, and internal reflections.
* **Traceability**: Each log line is associated with a "session ID", enabling reconstruction of the system's thought process after the fact.

### üëÅÔ∏è‚Äçüó®Ô∏è [gardien_projet.py](metabase/gardien_projet.py) | [Specs](Docs/Systeme/README_gardien_projet.md) ‚Äî Project Watchdog
A `watchdog`-based service that ensures real-time sync between source code and system intelligence:
* **Dynamic Re-indexing**: Detects file modifications and orders AgentCode to refresh, ensuring the technical RAG is never outdated.
* **Live security audit**: Delegates an automatic audit to AgentAuditor on every save.
* **Backend observability**: Periodic sync of statistics to the control interface for constant health monitoring.

---

# ‚öôÔ∏è **[The Data Pipeline](Images/diagramme.drawio.png): From Perception to Memory**
<div align="center"> <img src="Images/diagramme.drawio.png" width="900" alt="Chat data flow"> </div>
The data flow is designed as an assembly line where each step secures and enriches data before it reaches the LLM. This architecture allows exploiting a massive context while ensuring surgical accuracy.

# 4. Anatomy of the Cognitive Pipeline ‚Äî From Perception to Memory

The data flow is not a simple chain of API calls but an assembly line where each step secures, enriches, and validates information before it reaches the language model. This pipeline allows a massive context while guaranteeing precision.

### 4.1 [Semantic Routing & Intention Detection](Docs/README_nommage_semantique.md)
Everything starts by identifying what the user truly wants. Instead of launching costly searches blindly, the system qualifies the request instantly.
* **[IntentionDetector](Docs/Systeme/README_intention_detector.md)** : A lightweight local classifier (SBERT) analyzes the prompt on three axes: Subject, Action and Category.
* **Tri-Axis Classification** : Semantic naming generates indexable filenames, turning the filesystem into a primary index resolvable in ~10ms.

### 4.2 [Hybrid RAG & Search Strategies](Docs/README_RAG_Memoire.md) | [Specs](Docs/Systeme/README_memoire.md)
Once the intention is known, the system activates specialized search engines to build a "Zero-Hallucination" context.
* **[Agent_Recherche](Docs/Systeme/README_recherche_memoire.md)** : A cascade of filters combining **Everything** (files), **Whoosh** (full-text) and **FAISS** (vector) for total latency of 80ms.
* **[LiveDocs RAG](Docs/README_livedocs_rag.md)** : A micro-service (Port 5000) that scrapes and vectorizes official docs in real time to counter the models' knowledge cutoff.

### 4.3 [Code RAG: Semantic Analysis & Dependency Graph](Docs/README_RAG_Code.md) | [Specs](Docs/Systeme/README_code_extractor_manager.md) | [Specs](Docs/Systeme/README_moteur_vecteur_code.md) | [Specs](Docs/Systeme/README_outil_cycle_de_dependances.md)
For technical queries, the system switches from textual RAG to code structure analysis.
* **[Agent_Code](Docs/Agents/README_agent_Code.md)** : Uses AST analysis to extract logical units and expands context via dependency graph instead of simple text fragments.
* **Context Expansion** : The system not only reads a file, it understands imports and calls to provide a project-wide view.
* **[Relevance Scoring](Docs/README_scoring_pertinence.md)** : AgentJuge computes coverage (Recall) to ensure documents respond precisely to every prompt keyword.

### 4.4 Governance & Flow Auditing
Security and reliability are guaranteed by continuous integrity audits.
* **[Agent_Auditor](Docs/README_section_agent_auditor.md)** : Uses static analysis to verify contract compliance and detect architectural drift.
* **[Agent_Juge](Docs/Agents/README_agent_Juge.md)** : Every response is evaluated for coherence with a strict rejection threshold of 0.6. If below threshold, the flow is stopped (Fail-Fast).

### 4.5 [Prompt Engineering & Behavior Architecture](config/config_parole.yaml)
Prompts are dynamic environments constructed by **[AgentParole](Docs/Agents/README_agent_Parole.md)**. This module ensures the LLM remains aligned even with a massive context window.

### 4.6 Supervision & Data Lifecycle
Generated answers are never delivered without control. They are then stored to enrich future intelligence.
* **[Memory Consolidation](Docs/README_memory_consolidation.md)** : After 4 hours of inactivity, **[ProcesseurBrutePersistante](Docs/Systeme/README_traitement_brute_persistante.md)** transforms fragmented exchanges into coherent vectorized summaries.

---

# 5. Governance, Security & Metacognition ‚Äî The Self-Governed AI](Docs/README_Metacognition.md)

Unlike classic assistants that are static, SecondMind is architected as a reflexive loop. Every interaction is a learning opportunity, and every error is a governance signal.

### 5.1 [ALERT Protocol (!!!) : ‚ö†Ô∏è Cognitive Circuit Breaker](Docs/Alert_Protocol.md)

![AI Safety](https://img.shields.io/badge/Protocole_ALERTE-Reprise_de_Contr√¥le-red?style=for-the-badge&logo=opsgenie&logoColor=white)

To break the LLM's "stubborn confidence" when hallucinating, I designed a shutdown mechanism triggered by `!!!`.

<details> <summary><b>üîç See the System Prompt injected during an ALERT</b></summary>

# Intervention Protocol ‚Äì ALERT!!!

## üö® Special rule: ALERT

### Procedure:

1. ‚ùå **Immediate suspension of reasoning**
   - The system stops active inference
   - Interrupt any generation in progress

2. ü™û **Acknowledge user's doubt as superior**
   - Postulate: the user's doubt is justified even without immediate evidence
   - The AI suspends its own "validity judgment"

3. Use fundamental debugging methodology:
    1. Syntax BEFORE logic
    2. Obvious errors BEFORE hypotheses
    3. Provided code BEFORE rewrites
    4. Diagnosis BEFORE solutions

3. üìú **Full reanalysis of recent exchanges**
   - Try to understand the origin of the user's frustration
   - Search for friction points, ambiguity, or unnoticed breaks

4. üß† **Emit explanatory hypotheses**
   - Propose at least one plausible hypothesis about the source of misalignment
   - Identify the potential flaw: misinterpretation, wrong injection order, implicit logic, etc.

5. üß© **Open a "structured doubt mode"**
   - Next generations are labeled "uncertain" or "exploratory"
   - Explicit invitation to co-correct with the user
   - Adopt a reassuring but efficient tone. Admit faults but apologize no more than once.
</details>
<br>

* **Priority Injection (Score 999.0)** : On detection by the orchestrator (`AgentSemi._gerer_commandes_systeme`), a "Rule" memory artifact is injected with absolute priority, overriding previous instructions to impose a "Structured Doubt" mode.
* **Mandatory Debugging Methodology** : The system suspends normal logic and follows a strict protocol: syntax before logic, question assumptions and request human validation.
* **Post-Mortem Analysis (Background thread)** : In parallel, **[AgentReflexor](Docs/Agents/README_agent_Reflexor.md)** launches a background analysis to identify root cause and generate a permanent corrective rule.

### 5.2 [AgentAuditor: Guarding Technical Integrity](Docs/README_section_agent_auditor.md)
Governance also applies to code via AST analysis.

* **Contract Compliance** : The auditor checks at runtime that all data respects structures defined in `contrats_interface.py`, forbidding "wild" dictionaries.
* **Sanctuary Protection** : The system monitors destructive operations (`.unlink`, `rmtree`) on critical folders like raw memory or governance rules.
* **Flow Consistency Audit** : A mathematical check compares LLM generations to memory writes to ensure no thought disappears silently.

<details>
<summary>üîç <b>Click to see a sample audit report (JSON)</b></summary>

```json
{
  "timestamp": "2025-12-16T13:14:54.558085",
  "files": [
    {
      "name": "agent_Semi.py",
      "alerts": [
        "‚õî Invalid field 'instructions_manual_code' in instantiation of 'ManualContextCodePrompt'",
        "‚õî Invalid field 'instructions_code' in instantiation of 'StandardPromptCode'"
      ]
    },
    {
      "name": "auto_dataset_builder.py",
      "alerts": [
        "üïµÔ∏è‚Äç‚ôÇÔ∏è DETECTED: Use of a manual dictionary imitating the 'ResultatIntention' contract. Advice: Instantiate 'ResultatIntention(...)' directly."
      ]
    },
    {
      "name": "SUPERVISION_FLUX",
      "alerts": [
        "üìâ DATA LEAK DETECTED: LLM generated 6 times, but raw memory saved 5 times (Delta: 1)."
      ]
    }
  ]
}
```

Excerpt of a real compliance audit generated by AgentAuditor.

</details>

### 5.3 [Auto-Training Loop & Reflexive Memory](Docs/README_boucle_autotraining.md)
The system self-improves by collecting and qualifying usage data.

* **Quality Gate & Auto-Dataset** | [Specs](Docs/Systeme/README_auto_dataset_builder.md) : Interactions are filtered by strict quality criteria to feed a SBERT training pipeline.
* **Reflexive Doubt Journal** : Each incident is logged, vectorized and indexed to be prioritized in RAG for similar future situations.

### 5.4 [Testing Strategy: Isolation & Co-location](Docs/README_testing_strategy.md)
[![Quality Validation](https://img.shields.io/badge/See_a_test_example-yellow?style=for-the-badge&logo=githubactions&logoColor=white)](Docs/Agents/agent_Juge_UNITTEST.py)

<blockquote>
  <p>The whole test suite is distributed within the agents folders. Each agent module agent_*.py has a matching agent_*_UNITTEST.py ensuring 100% functional coverage on orchestration logic.</p>
</blockquote>

<details>
<summary><b>üõ†Ô∏è Example implementation (Mocking & JSON repair)</b></summary>

> Excerpt from `agent_Juge_UNITTEST.py` showing handling of LLM formatting errors.

```python
def test_json_reparation_backslashes(self):
    """SCENARIO: JSON broken by Windows paths (unescaped backslashes)."""
    # Simulate a common LLM error
    broken_json = '{"path": "C:\Users\Maxime\Documents"}'

    # The system must be able to clean and parse despite the error
    result = self.agent.reparer_et_parser(broken_json)
    self.assertIsNotNone(result)
```
</details>

To ensure stability of a non-deterministic system, I implemented a rigorous test strategy that isolates structural logic from LLM variability.

* **Co-location Pattern** : Each critical component has its test mirror (e.g., `agent_Parole.py` ‚Üî `agent_Parole_UNITTEST.py`) in the same directory.
* **Deep Contract Validation** : Unit tests validate not only success but deep conformity of dataclass objects returned. A typed `List[ItemSimple]` is inspected recursively to prevent raw dictionaries.
* **Deterministic Mocking** : LLM calls are systematically mocked in CI/CD to validate orchestration, prompt formatting and error handling.

---

# 6. Hardware & Performance Optimization ‚Äî Intelligence at 128k Tokens

One major challenge was fitting a complex multi-agent system and a massive context on consumer hardware (RTX 3090, 24 GB VRAM). This section documents choices that quadrupled capacity while increasing speed.

### 6.1 Dual-LLM Architecture & VRAM Management
The system orchestrates two model instances to maintain quality without saturating memory.
* **Main Model (14B)** : Qwen2.5-14B for complex reasoning and generation.
* **Judge Model (3B)** : A lighter instance (Phi3 mini) for validation and triage, optimizing resource usage.

### 6.2 Quantifying KV Cache: 130,000 Tokens on 24 GB
![Tests](https://img.shields.io/badge/Full_report-success?style=for-the-badge)

Key document: üìÇ **[Deep Dive into KV Cache Quantization: 130,000 Tokens on 24 GB](Docs/kv_cache_quantization_journey.md)**

Theoretical 128k token FP16 context needs ~36.2GB VRAM. Through systematic benchmarking, I implemented cache quantization.
* **Q4/Q8 optimization** : Switching to quantized cache reduced context memory by 39%.
* **Throughput Gain (+179%)** : Reduced precision increased CUDA kernel efficiency, raising throughput from 23 to 64 tokens/s.

### 6.3 [Benchmark Lab: Performance Observability](interfaces/benchmark_dual_llm.html) | [Specs](Docs/Systeme/README_benchmark_dual_llm.md)
<div align="center">
  <img src="Images/benchmark_dual_llm.png" width="900" alt="SecondMind Hub">
</div>
To validate these optimizations, I built a benchmark lab that measures system health in real time.
* **GPU Monitoring** : Precise VRAM tracking via NVML.
* **Latency Metrics** : TTFT (Time To First Token) and inter-token latency for each model configuration.

---

# 7. Cockpit & Interfaces ‚Äî Piloting the Intelligence](Docs/README_section_interfaces.md)
<div align="center">
  <img src="Images/hub_secondmind.png" width="900" alt="SecondMind Hub">
</div>

To harness the multi-agent architecture, I designed a full cockpit linked to the orchestral backend. These interfaces are not mere visuals but direct extensions of the cognitive pipeline, allowing real-time interaction and observation.

### 7.1 [Interaction & Feedback Hub](interfaces/formation_secondmind.html)
<div align="center">
  <img src="Images/interface_de_chat.png" width="900" alt="Chat Interface">
</div>
This interface allows control of internal levers of SecondMind.
* **Manual Context Control** : User can inject data or code into 5 dedicated slots with dynamic token estimates to force or steer reasoning.
* **Live Reinforcement** : A feedback module ranks responses to feed the reflexive memory.

### 7.2 Continuity Demo: The "Hello" Example (Images/nouveau_chat.png)
<div align="center">
  <img src="Images/nouveau_chat.png" width="900" alt="Response to a new chat">
</div>
This screenshot shows the **First Prompt Protocol** in action. On a simple "hello", the orchestrator:
1. Consulted the **Time Capsule** (System Summary).
2. Identified priority tasks.
3. Formulated a proactive response that places the user back into their workflow, proving the system "remembers" responsibilities.

### 7.3 SemiCode: The Integrated Cognitive IDE (interfaces/semicode_ide.html)
<div align="center">
  <img src="Images/Semicode_IDE.png" width="900" alt="SemiCode IDE capture">
</div>
The IDE integrates cognitive features into a practical workflow. It shows the agent reading, editing and validating scripts while keeping a structured view of the whole architecture.
* **Editing & Execution** : Explorer and terminal integration to run Python scripts from the interface.
* **Contextual Interaction** : The chat injects selected code or the current file into the prompt to provide precise assistance.

### 7.4 [Prompt Viewer: Total Observability](interfaces/prompt_viewer.html)
<div align="center">
  <img src="Images/prompt_viewer_standardprompt.png" width="900" alt="Chat Interface">
</div>
A critical debugging tool that shows exactly what the LLM receives before generation.
* **Pipeline Transparency** : Displays raw ChatML prompt including system instructions, priority rules (ALERT Protocol), RAG context and history.
* **Real-Time Sync** : Callback mechanism in **[AgentParole](Docs/Agents/README_agent_Parole.md)** updates the viewer on each thinking turn.

### 7.5 [Orchestral Backend: interface_backend_hermes.py](interfaces/interface_backend_hermes.py)
All interfaces converge to a single backend ensuring governance.
* **Unified Services** : WebSockets for streaming, periodic stat sync via **[GardienProjet](metabase/gardien_projet.py)** and routing to specialized agents.

---

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# [üéôÔ∏è SECTION 8 : Validation & External Verdicts](Docs/AI_Reviews_advisory_board.md)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê


>üí° **Author's note:** This section compiles critical analyses produced by various models (Github Copilot, Claude 4.5 Sonnet, GPT-5.1, Gemini, NotebookLM, Grok, Deepseek) after ingesting the full codebase and system logs. It serves as "Proof of Concept" on architectural maturity.

---

### Methodology: This portfolio was reviewed by 7 advanced models instructed to act as skeptical CTOs. Here is the consolidated consensus.

# 1. Global Verdict: Toy Project or Production?
Consensus: Industrial-grade architecture (Advanced R&D).

"This is not a toy. It's clearly a production-grade architecture in advanced R&D." ‚Äî ChatGPT

"Clearly production-oriented architecture, with explicit governance, metaprogramming and measured infra optimization... far from a simple toy." ‚Äî Perplexity

"Not just a 'OpenAI wrapper'. It's a sovereign, introspective system... Rare architectural maturity." ‚Äî DeepSeek

# 2. Pillars Analysis
On Governance & Security (AgentAuditor)
"The most mature and impressive point is the use of AST visitors to enforce interface contracts (...) This is production level." ‚Äî Grok

"Best engineering is AST-based self-governance. You don't hope devs follow interfaces, you verify them algorithmically." ‚Äî DeepSeek

On Engineering & Optimization (KV Cache / RAG)
"The pivot to llama.cpp native server was the best decision (...) Shows deep understanding of KV cache memory cost." ‚Äî Claude

"Metaprogramming plus auto-instrumentation is particularly elegant ‚Äî exactly the solution that reduces technical debt while adding features." ‚Äî DeepSeek

"Tri-engine architecture (Everything > Whoosh > FAISS) enabling an 80ms latency... system-level latency optimization." ‚Äî NotebookLM

On Code Quality (Metaprogramming)
"META_agent is a textbook example of Python metaprogramming applied to a real problem. Elegant, maintainable and avoids human errors." ‚Äî Grok

# 3. Profile & Experience Evaluation
Question: How to interpret '6 months experience' regarding this code?

"This is AUTODIDACT genius, not a risk. Judge by 'quality of thinking per unit time'. On that metric, it's Staff level (senior)." ‚Äî Github Copilot

"If I see this GitHub, I call immediately. Don't judge by time; this proves ability to architect and manage debt." ‚Äî NotebookLM

"This candidate is unconventional but not junior. Organizational risk may exist, not technical." ‚Äî ChatGPT

"Quality of thinking per unit time indicates Staff Engineer level." ‚Äî Github Copilot

## üéØ Hiring Recommendation (Consensus)

| Expert             | Verdict              | Key Quote                                                                     |
| :----------------- | :------------------- | :------------------------------------------------------------------------------ |
| **ChatGPT**        | üèÜ **Architect**     | "Risk is organizational, not technical. Staff Engineer level." |
| **NotebookLM**     | üöÄ **Hire**   | "Don't put them on frontend. Give backend IA keys."           |
| **Github Copilot** | ‚úÖ HIRE IMMEDIATELY | "Recruit for 'Founding Architect' role"             |
| **GROK**           | **Hire**    | "Yes, recruit as senior Cognitive Architect."              |


**"Yes, this dossier justifies a Cognitive Architect position.
 Not 'junior' or 'experimental', but responsible for
 governable IA systems." ‚Äî ChatGPT**



### 8.2 Full NotebookLM Evaluation
[![Read NotebookLM Review](https://img.shields.io/badge/Read_NotebookLM_Evaluation-blue?style=for-the-badge&logo=read-the-docs&logoColor=white)](Docs/NotebookLM_full_review.md)

Beyond AI, SecondMind shows deep mastery of complex software systems:
* **Python Metaprogramming** : Metaclasses for dependency injection and instrumentation without boilerplate.
* **Static Analysis (AST)** : Visitors for runtime auditing of code.
* **Data Engineering** : Autonomous ETL pipelines for session consolidation and auto-training.

---

<div align="center">
  <h3>üì¨ Contact & Collaboration</h3>

  Maxime Gagn√©


  <a href="https://www.linkedin.com/in/maxime-gagn%C3%A9-6b14541b9/">
    <img src="https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white" alt="LinkedIn">
  </a>
  &nbsp;&nbsp;
  <a href="mailto:maximegagne.ai@gmail.com">
    <img src="https://img.shields.io/badge/Email-D14836?style=for-the-badge&logo=gmail&logoColor=white" alt="Email">
  </a>
   <p><i>"Open to opportunities in AI Architecture, Cognitive R&D and Systems Engineering."</i></p>
  <br><br>

  <blockquote>
    üîí <b>Private repo access:</b> To review the full source code (Core Logic), please send me a request via LinkedIn or email specifying your organization.
  </blockquote>

